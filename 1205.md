# ACE Framework Deep Dive - December 5, 2024

This document summarizes the key concepts and implementation details of the Agentic Context Engineering (ACE) framework.

## Table of Contents
1. [The Improvement Signal & Objective](#the-improvement-signal--objective)
2. [The Update Mechanism](#the-update-mechanism)
3. [How Environments Work](#how-environments-work)
4. [Metrics and Score Ranges](#metrics-and-score-ranges)
5. [How Feedback Strings Are Used](#how-feedback-strings-are-used)
6. [Creating Feedback Strings](#creating-feedback-strings)
7. [Multi-Step Feedback](#multi-step-feedback)

---

## The Improvement Signal & Objective

### What is the Objective?

The objective is to improve agent performance on tasks by learning reusable **skills** (strategic knowledge) from execution feedback. The agent learns what works and what doesn't, storing this in a **skillbook** that guides future behavior.

### Where Does the Improvement Signal Come From?

The improvement signal comes from the `TaskEnvironment.evaluate()` method, which returns an `EnvironmentResult`:

```python
@dataclass
class EnvironmentResult:
    feedback: str                    # Human-readable explanation of outcome
    ground_truth: Optional[str]      # The correct answer (for Reflector)
    metrics: Dict[str, float] = {}   # Quantitative scores (for analytics)
```

### The Learning Loop

```
┌─────────────────────────────────────────────────────────────────┐
│                     ACE IMPROVEMENT LOOP                        │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. Agent attempts task using current skillbook                 │
│                    ↓                                            │
│  2. Environment evaluates output → returns FEEDBACK             │
│     • "Correct!" / "Incorrect. Expected X"                      │
│     • ground_truth (optional)                                   │
│     • metrics: {"accuracy": 1.0, "score": 0.85}                │
│                    ↓                                            │
│  3. Reflector analyzes: What worked? What failed?               │
│     • Classifies which skills were helpful/harmful              │
│     • Suggests new skills to add                                │
│                    ↓                                            │
│  4. SkillManager updates skillbook                              │
│     • ADD new skills, UPDATE counters, REMOVE bad ones          │
│                    ↓                                            │
│  5. Loop back with improved skillbook                           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## The Update Mechanism

**This is NOT traditional ML training.** There's no loss function or backpropagation. Instead, it uses **LLM-driven symbolic updates**.

### The Skill Data Structure

```python
@dataclass
class Skill:
    id: str
    section: str
    content: str          # The actual strategy/knowledge
    helpful: int = 0      # Counter: times this helped
    harmful: int = 0      # Counter: times this hurt
    neutral: int = 0      # Counter: times it was irrelevant
```

### Four Update Operations

| Operation | What it does |
|-----------|--------------|
| **ADD** | Creates a new skill with content |
| **UPDATE** | Modifies an existing skill's content |
| **TAG** | Increments `helpful`, `harmful`, or `neutral` counters |
| **REMOVE** | Deletes a skill from the skillbook |

### The Learning Flow

```
┌──────────────────────────────────────────────────────────────────────────┐
│  1. REFLECTOR analyzes the execution result                              │
│     Input: task, agent answer, environment feedback, current skillbook   │
│     Output:                                                              │
│       - error_identification: "Agent used wrong date format"             │
│       - root_cause_analysis: "No skill for ISO date handling"            │
│       - correct_approach: "Should use YYYY-MM-DD format"                 │
│       - skill_classifications: [                                         │
│           {skill_id: "fmt-001", classification: "harmful"},              │
│           {skill_id: "fmt-002", classification: "helpful"}               │
│         ]                                                                │
│       - new_skill_recommendations: ["Always use ISO 8601 dates"]         │
├──────────────────────────────────────────────────────────────────────────┤
│  2. SKILL MANAGER converts reflection → update operations                │
│     Input: Reflector's analysis                                          │
│     Output: UpdateBatch with operations like:                            │
│       [                                                                  │
│         {type: "TAG", skill_id: "fmt-001", metadata: {harmful: 1}},      │
│         {type: "TAG", skill_id: "fmt-002", metadata: {helpful: 1}},      │
│         {type: "ADD", section: "Formatting", content: "Use ISO 8601..."}│
│       ]                                                                  │
├──────────────────────────────────────────────────────────────────────────┤
│  3. SKILLBOOK applies updates                                            │
│     skillbook.apply_update(batch)                                        │
│     → Counters updated, new skills added, bad skills removed             │
└──────────────────────────────────────────────────────────────────────────┘
```

### Key Insight: The "Training Objective" is Implicit

There's **no explicit loss function**. Instead:

1. **The Reflector LLM** judges what worked/failed based on environment feedback
2. **The SkillManager LLM** decides what updates to make
3. **The counters** (`helpful`/`harmful`) serve as a soft measure of skill quality
4. **Pruning**: Skills with high `harmful` counts can be removed

This is essentially **in-context learning via structured memory** - the LLM learns by accumulating symbolic knowledge, not by updating weights.

---

## How Environments Work

The environment is an **abstract interface** that you implement to define "what success means" for your task.

### Core Interface

```python
class TaskEnvironment(ABC):
    @abstractmethod
    def evaluate(self, sample: Sample, agent_output: AgentOutput) -> EnvironmentResult:
        """Compare agent's answer to ground truth, return feedback."""
```

### Built-in Environments

| Environment | Task Type | Key Metrics |
|-------------|-----------|-------------|
| **SimpleEnvironment** | Q&A | `correct` (substring match) |
| **FiNEREnvironment** | NER | `precision`, `recall`, `f1`, `exact_match` |
| **XBRLMathEnvironment** | Math | `exact_match`, `relative_error`, `within_1_percent` |
| **AppWorldEnvironment** | Agent execution | `task_success`, `api_success_rate` |

### Example: SimpleEnvironment

```python
class SimpleEnvironment(TaskEnvironment):
    def evaluate(self, sample: Sample, agent_output: AgentOutput) -> EnvironmentResult:
        answer = agent_output.final_answer.lower()
        truth = sample.ground_truth.lower()
        is_correct = truth in answer   # Simple substring check

        return EnvironmentResult(
            feedback="Correct!" if is_correct else f"Incorrect. Expected: {sample.ground_truth}",
            ground_truth=sample.ground_truth,
            metrics={"correct": 1.0 if is_correct else 0.0}
        )
```

### The Environment Flow

```
┌────────────────────────────────────────────────────────────────────────────┐
│                        ENVIRONMENT IN THE LOOP                             │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│  Sample                                                                    │
│  ├─ question: "What company did Elon Musk found in 2002?"                 │
│  ├─ context: "..."                                                        │
│  └─ ground_truth: "SpaceX"                                                │
│                                                                            │
│        ↓                                                                   │
│                                                                            │
│  Agent.generate() → AgentOutput(final_answer="Tesla")                     │
│                                                                            │
│        ↓                                                                   │
│                                                                            │
│  Environment.evaluate(sample, agent_output)                               │
│        │                                                                   │
│        ├─→ Compares "Tesla" to "SpaceX"                                   │
│        ├─→ feedback: "Incorrect. Expected: SpaceX"                        │
│        ├─→ metrics: {"correct": 0.0}                                      │
│        └─→ ground_truth: "SpaceX"                                         │
│                                                                            │
│        ↓                                                                   │
│                                                                            │
│  Reflector receives:                                                       │
│  - The question                                                            │
│  - Agent's wrong answer ("Tesla")                                         │
│  - Environment feedback ("Incorrect...")                                  │
│  - Ground truth ("SpaceX")                                                │
│  → Analyzes WHY the agent was wrong                                       │
│  → Suggests skills to add/update                                          │
│                                                                            │
└────────────────────────────────────────────────────────────────────────────┘
```

### Key Design Points

1. **You define success** - The environment is task-specific
2. **Feedback is for learning** - The `feedback` string goes to the Reflector
3. **Metrics are for analysis** - The `metrics` dict tracks quantitative progress
4. **Ground truth is optional** - Some tasks may only have success/failure signals

---

## Metrics and Score Ranges

### Important: Metrics Are NOT Used for Learning

Looking at the code, the **metrics** (`Dict[str, float]`) are **not passed to the Reflector or SkillManager**. Only the **feedback string** is used for learning:

```python
# From adaptation.py:435-442
env_result = environment.evaluate(sample, agent_output)
reflection = self.reflector.reflect(
    question=sample.question,
    agent_output=agent_output,
    skillbook=self.skillbook,
    ground_truth=env_result.ground_truth,   # ← Passed
    feedback=env_result.feedback,            # ← Passed (STRING)
    # metrics is NOT passed!
)
```

### Can We Use Unbounded Metrics Like PnL?

**Yes!** The learning happens through the **feedback string**, which you craft yourself:

```python
class TradingEnvironment(TaskEnvironment):
    def evaluate(self, sample: Sample, agent_output: AgentOutput) -> EnvironmentResult:
        pnl = calculate_pnl(agent_output.final_answer, sample.ground_truth)

        # The FEEDBACK STRING is what drives learning
        if pnl > 1000:
            feedback = f"Excellent trade! PnL: ${pnl:,.2f}. Strategy was highly profitable."
        elif pnl > 0:
            feedback = f"Profitable trade. PnL: ${pnl:,.2f}. Modest gains achieved."
        elif pnl > -500:
            feedback = f"Small loss. PnL: ${pnl:,.2f}. Risk management prevented major damage."
        else:
            feedback = f"Significant loss! PnL: ${pnl:,.2f}. Strategy failed badly."

        return EnvironmentResult(
            feedback=feedback,                    # ← THIS drives learning
            ground_truth=str(sample.ground_truth),
            metrics={"pnl": pnl, "roi": pnl/10000}  # ← For YOUR analytics only
        )
```

### The Learning Signal Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    WHAT ACTUALLY DRIVES LEARNING                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Environment.evaluate()                                                     │
│       │                                                                     │
│       ├─→ metrics: {"pnl": -2500, "sharpe": 0.3}                           │
│       │       └─→ Used for: YOUR dashboards, charts, analysis              │
│       │           NOT seen by: Reflector, SkillManager                     │
│       │                                                                     │
│       ├─→ feedback: "Significant loss! Strategy failed due to..."          │
│       │       └─→ Seen by: Reflector (analyzes what went wrong)            │
│       │           Drives: Skill tagging (helpful/harmful/neutral)          │
│       │           Drives: New skill suggestions                            │
│       │                                                                     │
│       └─→ ground_truth: "sell at $142.50"                                  │
│               └─→ Seen by: Reflector (compares to agent's answer)          │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## How Feedback Strings Are Used

The feedback is **injected directly into the Reflector's prompt** as plain text.

### Step 1: Feedback Goes Into Reflector Prompt

```python
# From roles.py:580-587
base_prompt = self.prompt_template.format(
    question=question,
    reasoning=agent_output.reasoning,
    prediction=agent_output.final_answer,
    ground_truth=_format_optional(ground_truth),
    feedback=_format_optional(feedback),      # ← YOUR FEEDBACK STRING
    skillbook_excerpt=skillbook_context,
)
```

### Step 2: The Reflector Sees This

From `prompts.py`, the Reflector prompt template:

```
You are a senior reviewer diagnosing the agent's trajectory.
Use the skillbook, model reasoning, and feedback to identify mistakes...

Question:
What trade should I make on AAPL?

Model reasoning:
Buy AAPL at $180, target $200, stop at $170...

Model prediction: BUY 100 shares AAPL at $180

Ground truth (if available): SELL - earnings miss incoming

Feedback: Significant loss! PnL: -$2,500. Held through earnings    ← YOUR FEEDBACK
         despite volatility warning. Position sizing was 2x normal.

Skillbook excerpts consulted:
[momentum-001] Follow trend direction (helpful=5, harmful=2)
[risk-003] Use 2% position sizing (helpful=3, harmful=0)

Return JSON:
{
  "reasoning": "<analysis>",
  "error_identification": "<what went wrong>",
  "root_cause_analysis": "<why it happened>",
  ...
}
```

### Step 3: Reflector LLM Produces Analysis

```json
{
  "reasoning": "Agent ignored earnings risk despite volatility warning in feedback",
  "error_identification": "Held long position through earnings announcement",
  "root_cause_analysis": "No skill for earnings-related risk management.
                          Position sizing skill [risk-003] was ignored (2x normal used)",
  "correct_approach": "Close or reduce positions before major announcements",
  "key_insight": "Earnings events require special position management",
  "skill_tags": [
    {"id": "momentum-001", "tag": "harmful"},
    {"id": "risk-003", "tag": "harmful"}
  ]
}
```

### Step 4: SkillManager Uses Reflection + Context

The SkillManager receives:
- The Reflector's analysis
- The `question_context` which includes your feedback again

```python
# From adaptation.py:401-411
def _question_context(self, sample: Sample, environment_result: EnvironmentResult) -> str:
    parts = [
        f"question: {sample.question}",
        f"context: {sample.context}",
        f"metadata: {json.dumps(sample.metadata)}",
        f"feedback: {environment_result.feedback}",      # ← FEEDBACK AGAIN
        f"ground_truth: {environment_result.ground_truth}",
    ]
    return "\n".join(parts)
```

### The Complete Feedback Flow

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     FEEDBACK STRING FLOW                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Environment.evaluate()                                                     │
│       │                                                                     │
│       └─→ feedback = "Loss of $2,500. Held through earnings..."            │
│                                                                             │
│              ↓                                                              │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  REFLECTOR PROMPT (text injection)                                  │   │
│  │                                                                     │   │
│  │  "...                                                               │   │
│  │   Feedback: Loss of $2,500. Held through earnings...               │   │
│  │   ...                                                               │   │
│  │   What went wrong? Why? What should be done?"                       │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│              │                                                              │
│              ↓ LLM generates                                               │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  REFLECTOR OUTPUT (structured JSON)                                 │   │
│  │                                                                     │   │
│  │  {                                                                  │   │
│  │    "error_identification": "Held through earnings",                 │   │
│  │    "root_cause_analysis": "No earnings risk skill",                 │   │
│  │    "key_insight": "Close positions before announcements",           │   │
│  │    "skill_tags": [{"id": "momentum-001", "tag": "harmful"}]        │   │
│  │  }                                                                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│              │                                                              │
│              ↓                                                              │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  SKILL MANAGER PROMPT                                               │   │
│  │                                                                     │   │
│  │  "Recent reflection: {above analysis}                               │   │
│  │   Question context: ...feedback: Loss of $2,500...                  │   │
│  │                                                                     │   │
│  │   What updates to make?"                                            │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│              │                                                              │
│              ↓ LLM generates                                               │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  SKILLBOOK UPDATES                                                  │   │
│  │                                                                     │   │
│  │  [                                                                  │   │
│  │    {type: "ADD", content: "Close positions before earnings"},       │   │
│  │    {type: "TAG", skill_id: "momentum-001", metadata: {harmful: 1}}  │   │
│  │  ]                                                                  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Key Insight: It's All Natural Language

The feedback is **not parsed programmatically** - it's read by an LLM. This means:

1. **Rich feedback = better learning** - More context helps the Reflector understand what happened
2. **Structure doesn't matter** - The LLM extracts meaning from natural language
3. **Domain-specific terms work** - Use your trading jargon, the LLM will understand

---

## Creating Feedback Strings

The feedback string is **created by you** when you implement the `TaskEnvironment.evaluate()` method.

### Pattern 1: Simple Correct/Incorrect

```python
class SimpleEnvironment(TaskEnvironment):
    def evaluate(self, sample: Sample, agent_output: AgentOutput) -> EnvironmentResult:
        is_correct = sample.ground_truth.lower() in agent_output.final_answer.lower()

        return EnvironmentResult(
            feedback="Correct!" if is_correct else f"Incorrect. Expected: {sample.ground_truth}",
            ground_truth=sample.ground_truth,
            metrics={"correct": 1.0 if is_correct else 0.0},
        )
```

### Pattern 2: Score-Based Tiers

```python
def evaluate(self, sample: Sample, agent_output) -> EnvironmentResult:
    score = self._compute_score(agent_output, sample)

    if score >= 0.8:
        feedback = f"Good performance ({score:.1%}). Answer aligns well with expected output."
    elif score >= 0.5:
        feedback = f"Moderate performance ({score:.1%}). Consider refining approach."
    else:
        feedback = f"Low performance ({score:.1%}). Significant improvement needed."

    return EnvironmentResult(feedback=feedback, ground_truth=sample.ground_truth, metrics={"score": score})
```

### Pattern 3: Rich Diagnostic Feedback

```python
def _generate_ner_feedback(self, predicted, gold, metrics):
    f1_score = metrics["f1"]
    precision = metrics["precision"]
    recall = metrics["recall"]

    feedback_parts = [f"F1: {f1_score:.2%}, Precision: {precision:.2%}, Recall: {recall:.2%}"]

    if f1_score >= 0.8:
        feedback_parts.append("Excellent entity recognition performance.")
    else:
        feedback_parts.append("Entity recognition needs improvement.")

    # Specific guidance
    if precision < recall:
        feedback_parts.append("Focus on reducing false positives.")
    elif recall < precision:
        feedback_parts.append("Focus on improving recall - find all entities.")

    # Show what was missed/wrong
    missed = gold - predicted
    if missed:
        feedback_parts.append(f"Missed {len(missed)} entities: {list(missed)[:3]}...")

    return " ".join(feedback_parts)
```

### Template for Custom Environments

```python
class TradingEnvironment(TaskEnvironment):
    def evaluate(self, sample: Sample, agent_output: AgentOutput) -> EnvironmentResult:
        # 1. Parse agent's decision
        decision = self._parse_decision(agent_output.final_answer)

        # 2. Calculate outcome metrics
        pnl = self._calculate_pnl(decision, sample)
        max_drawdown = self._calculate_drawdown(decision, sample)

        # 3. Build feedback string (THIS IS WHAT YOU DESIGN)
        feedback_parts = []

        # Outcome summary
        if pnl > 0:
            feedback_parts.append(f"Profitable trade: ${pnl:+,.2f}")
        else:
            feedback_parts.append(f"Loss: ${pnl:,.2f}")

        # What happened
        feedback_parts.append(f"Entry: ${decision.entry:.2f}, Exit: ${decision.exit:.2f}")

        # Domain-specific warnings
        if sample.metadata.get("had_earnings"):
            feedback_parts.append("Trade spanned earnings announcement")
        if max_drawdown > 0.1:
            feedback_parts.append(f"High drawdown: {max_drawdown:.1%}")

        # Guidance for improvement
        if pnl < 0 and sample.metadata.get("had_earnings"):
            feedback_parts.append("Consider: Reduce position or exit before earnings")

        feedback = "\n".join(feedback_parts)

        return EnvironmentResult(
            feedback=feedback,
            ground_truth=sample.ground_truth,
            metrics={"pnl": pnl, "max_drawdown": max_drawdown}
        )
```

### Key Principle

**The richer and more diagnostic your feedback, the better the Reflector can learn.** Include:

| What to Include | Example |
|-----------------|---------|
| **Outcome** | "Loss of $2,500" |
| **What happened** | "Held through earnings, gap down on open" |
| **Why it matters** | "Position was 2x normal size" |
| **What to do differently** | "Consider reducing before binary events" |

---

## Multi-Step Feedback

There are **two approaches** for multi-step feedback:

### Approach 1: Rich Trace in Feedback String (Built-in for LangChain)

The `ACELangChain` integration captures multi-step traces from LangChain's `AgentExecutor`:

```python
def _learn_with_trace(self, original_input: Any, result: Dict[str, Any]):
    """Learn from AgentExecutor result with intermediate_steps."""

    steps = result.get("intermediate_steps", [])

    # Build rich reasoning from steps
    parts = [f"=== AGENT EXECUTION TRACE ({len(steps)} steps) ==="]

    for i, (action, observation) in enumerate(steps, 1):
        parts.append(f"\n--- Step {i} ---")
        if hasattr(action, "log"):
            parts.append(f"Thought: {action.log}")
        if hasattr(action, "tool"):
            parts.append(f"Action: {action.tool}")
            parts.append(f"Action Input: {action.tool_input[:300]}")
        parts.append(f"Observation: {str(observation)[:300]}")

    parts.append("\n=== END TRACE ===")
    parts.append(f"\nFinal Answer: {result.get('output', '')}")

    reasoning = "\n".join(parts)

    # This rich trace goes to the Reflector
    agent_output = AgentOutput(reasoning=reasoning, ...)
    feedback = f"Agent completed task in {len(steps)} steps"
```

### Approach 2: Build Your Own Multi-Step Feedback

For custom environments, construct the feedback string yourself:

```python
class TradingEnvironment(TaskEnvironment):
    def evaluate(self, sample: Sample, agent_output: AgentOutput) -> EnvironmentResult:
        trades = sample.metadata.get("execution_log", [])

        feedback_parts = []
        feedback_parts.append(f"=== TRADE EXECUTION LOG ({len(trades)} actions) ===\n")

        cumulative_pnl = 0
        for i, trade in enumerate(trades, 1):
            feedback_parts.append(f"--- Step {i}: {trade['action']} ---")
            feedback_parts.append(f"  Time: {trade['timestamp']}")
            feedback_parts.append(f"  Asset: {trade['symbol']}")
            feedback_parts.append(f"  Price: ${trade['price']:.2f}")
            feedback_parts.append(f"  Size: {trade['quantity']} shares")
            feedback_parts.append(f"  Reason: {trade.get('reasoning', 'N/A')}")

            step_pnl = trade.get('realized_pnl', 0)
            cumulative_pnl += step_pnl
            if step_pnl != 0:
                feedback_parts.append(f"  Step PnL: ${step_pnl:+,.2f}")

            if trade.get('market_reaction'):
                feedback_parts.append(f"  Market Reaction: {trade['market_reaction']}")

            if trade.get('issue'):
                feedback_parts.append(f"  Issue: {trade['issue']}")

            feedback_parts.append("")

        feedback_parts.append("=== END LOG ===\n")
        feedback_parts.append(f"Final PnL: ${cumulative_pnl:+,.2f}")

        if cumulative_pnl < 0:
            worst_step = self._find_worst_step(trades)
            feedback_parts.append(f"\nLargest loss at Step {worst_step['step']}: {worst_step['diagnosis']}")

        return EnvironmentResult(
            feedback="\n".join(feedback_parts),
            ground_truth=sample.ground_truth,
            metrics={"pnl": cumulative_pnl, "trades": len(trades)}
        )
```

### Example Multi-Step Feedback Output

```
=== TRADE EXECUTION LOG (4 actions) ===

--- Step 1: BUY ---
  Time: 2024-01-15 09:30:00
  Asset: AAPL
  Price: $185.50
  Size: 100 shares
  Reason: Momentum breakout above resistance

--- Step 2: HOLD ---
  Time: 2024-01-15 14:00:00
  Reason: Waiting for target price
  Issue: Earnings announcement in 2 hours

--- Step 3: HOLD ---
  Time: 2024-01-15 16:00:00
  Reason: After-hours earnings release
  Market Reaction: Gap down 8% on revenue miss

--- Step 4: SELL ---
  Time: 2024-01-16 09:30:00
  Asset: AAPL
  Price: $170.25
  Size: 100 shares
  Step PnL: -$1,525.00

=== END LOG ===

Final PnL: -$1,525.00

Largest loss at Step 3: Held through earnings despite warning at Step 2
```

### What the Reflector Can Extract

The Reflector analyzes the WHOLE trace as natural language and can:

1. **Identify which step went wrong** - "Error at Step 2: Should have exited before earnings"
2. **Tag skills per step** - "Momentum strategy (Step 1) was fine, but risk management (Step 2-3) failed"
3. **Extract step-specific learnings** - "Add skill: Exit positions before binary events"

### Pattern Summary

| Approach | When to Use | How |
|----------|-------------|-----|
| **Single feedback** | Simple tasks, Q&A | Return one summary string |
| **Rich trace in reasoning** | LangChain agents | Captured via `intermediate_steps` |
| **Multi-step in feedback** | Custom agents | Build the trace string yourself |
| **Hybrid** | Complex workflows | Trace in `reasoning` + summary in `feedback` |

---

## Summary

The ACE framework implements self-improving agents through:

1. **Environment feedback** - You define what success means
2. **LLM-based reflection** - The Reflector analyzes what worked/failed
3. **Symbolic updates** - The SkillManager updates a structured skillbook
4. **Natural language signals** - Everything flows through text, not gradients

The key insight is that **learning quality depends on feedback quality**. The richer and more diagnostic your feedback strings, the better the system can learn from experience.
